# WISER Ambiguity Clarification

A comprehensive research project investigating how Large Language Models (LLMs) can improve information retrieval systems by detecting and clarifying ambiguous queries. This project explores three complementary approaches to develop practical, cost-effective solutions for handling ambiguous queries in conversational search and question-answering systems.

## ğŸ¯ Goals

1. **Direct LLM Evaluation**: Assess various LLMs' capability to detect query ambiguity using zero-shot and few-shot prompting
2. **Knowledge Distillation**: Train smaller, efficient models (66M-125M parameters) using synthetic data generated by larger LLMs
3. **Query Clarification & Retrieval**: Use LLMs to clarify ambiguous queries and evaluate the impact on document retrieval performance


## ğŸ“Š Project Components

### 1. ambig2doc/ - Query Clarification & Document Retrieval

Uses LLMs to clarify ambiguous queries and evaluates the impact on retrieval performance using BM25-based search.

**Key Features:**
- Three prompting strategies: zero-shot, few-shot, and Ambig2doc (query + hypothetical document)
- BM25 retrieval evaluation on BEIR benchmark datasets (SciFact, SciDocs)
- Metrics: nDCG@10, Recall@100, Recall@1000

**Best Results:**
- **SciFact**: nDCG@10=0.7052, R@1000=1.0000 (Llama-3.3-70B + Ambig2doc)
- **SciDocs**: nDCG@10=0.1559, R@1000=0.6261 (Llama-3.3-70B + Ambig2doc)

**Key Scripts:**
- `clarification.py` - Main query clarification pipeline
- `run_all_retrievals.py` - Batch BM25 retrieval
- `evaluate_runs.py` - Compute retrieval metrics
- `generate_metrics_table.py` - Generate comparison tables

[ğŸ“– Detailed Documentation](ambig2doc/)

---

### 2. intent_cf/ - Synthetic Data Generation & PLM Fine-tuning

Investigates whether smaller Pre-trained Language Models can match larger LLMs on ambiguity detection when trained on LLM-generated synthetic data.

**Pipeline:**
1. **Data Generation**: Use LLMs to generate balanced synthetic training data (~1,000 samples)
2. **Model Training**: Fine-tune DistilBERT (66M) and RoBERTa (125M) with 5-fold cross-validation
3. **Evaluation**: Test on real-world datasets (ClariQ, AmbigNQ)

**Best Results:**
- **ClariQ** (Weighted F1): RoBERTa + GPT-4.1 data = **0.83** (vs 0.11 baseline)
- **AmbigNQ** (Weighted F1): RoBERTa + Llama-3.3-70B data = **0.52** (vs 0.42 baseline)
- **Inference**: ~2ms per sample (8x faster than direct LLM inference)

**Key Scripts:**
- `synthgen/generate_training_data.py` - Generate synthetic data
- `plm_training/distillbert.py` - Fine-tune DistilBERT
- `plm_training/roberta.py` - Fine-tune RoBERTa
- `scripts/run_all_generations.sh` - Batch generation

[ğŸ“– Detailed Documentation](intent_cf/README.md)

---

### 3. llm_direct_evaluation/ - Direct LLM Evaluation

Evaluates various LLMs' capability to detect ambiguous queries using zero-shot and few-shot prompting.

**Ambiguity Types:**
- UNFAMILIAR - Unknown/mixed entities
- CONTRADICTION - Logically inconsistent
- LEXICAL - Multiple word meanings
- SEMANTIC - Unclear/implausible meaning
- WHO/WHEN/WHERE/WHAT - Missing context

**Best Results:**
- **ClariQ** (Weighted F1): Llama-3.2-3B (zero-shot) = **0.826**
- **AmbigNQ** (Weighted F1): Llama-3.3-70B (zero-shot) = **0.541**

**Key Scripts:**
- `core/evaluate.py` - Main evaluation pipeline
- `scripts/run_all_evals.sh` - Batch evaluation

[ğŸ“– Detailed Documentation](llm_direct_evaluation/README.md)

## ğŸ“ˆ Performance Summary

| Approach | Method | Best Model | ClariQ F1 | AmbigNQ F1 | Inference Speed |
|----------|--------|------------|-----------|------------|-----------------|
| **Direct LLM** | Zero-shot | Llama-3.2-3B | 0.826 | 0.451 | ~100ms |
| **Direct LLM** | Zero-shot | Llama-3.3-70B | 0.662 | 0.541 | ~200ms |
| **Distilled PLM** | Fine-tuned | RoBERTa + GPT-4.1 | 0.830 | 0.490 | ~2ms |
| **Distilled PLM** | Fine-tuned | RoBERTa + Llama-3.3-70B | 0.790 | 0.520 | ~2ms |
| **Baseline** | Few-shot | Random | 0.110 | 0.420 | N/A |

**Key Insight**: Distilled small models (125M params) achieve comparable performance to large LLMs (3B-70B params) with 50-100x speedup.

## ğŸ¤– Supported Models

### Large Language Models
| Model | Parameters | Provider | Use Case |
|-------|------------|----------|----------|
| Llama-3.1-8B-Instruct | 8B | Meta | Query clarification, data generation |
| Llama-3.2-1B/3B-Instruct | 1B/3B | Meta | Efficient ambiguity detection |
| Llama-3.3-70B-Instruct | 70B | Meta | High-quality data generation |
| Gemma-3-1B/4B/27B-IT | 1B/4B/27B | Google | Data generation, evaluation |
| Phi-3-mini-128k | 4B | Microsoft | Evaluation |
| GPT-4.1 series | Proprietary | OpenAI | Data generation |

### Small Pre-trained Models
| Model | Parameters | Architecture | Training Time |
|-------|------------|--------------|---------------|
| DistilBERT | 66M | 6-layer distilled BERT | ~30 min/fold |
| RoBERTa-base | 125M | Robustly optimized BERT | ~45 min/fold |

## ğŸ“š Datasets

### Evaluation Datasets
| Dataset | Description | Size | Labels | Domain |
|---------|-------------|------|--------|--------|
| **ClariQ** | Conversational queries needing clarification | ~300 | Binary | Conversational search |
| **AmbigNQ** | Ambiguous natural questions | ~2,000 | Binary | Open-domain QA |
| **SciFact** | Scientific fact verification | Test set | Relevance | Scientific papers |
| **SciDocs** | Scientific document retrieval | Test set | Relevance | Scientific papers |

### Synthetic Training Data
- LLM-generated balanced datasets (~1,000 samples per model)
- 100+ diverse topic categories (Banking, Health, Travel, Technology, etc.)
- 50% ambiguous / 50% unambiguous


## ğŸ“ Repository Structure

```
wiser_ambiguity_clarification/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Global dependencies
â”‚
â”œâ”€â”€ ambig2doc/                         # Query clarification & retrieval
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ clarification.py              # Main clarification pipeline
â”‚   â”œâ”€â”€ prompts.py                    # Prompt templates
â”‚   â”œâ”€â”€ run_all_retrievals.py         # BM25 retrieval
â”‚   â”œâ”€â”€ evaluate_runs.py              # Metrics computation
â”‚   â”œâ”€â”€ generate_metrics_table.py     # Results visualization
â”‚   â”œâ”€â”€ queries/                      # Clarified query outputs
â”‚   â”œâ”€â”€ runs/                         # Retrieval run files
â”‚   â””â”€â”€ llm/                          # Model implementations
â”‚
â”œâ”€â”€ intent_cf/                         # Synthetic data & PLM training
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ synthgen/                     # Data generation
â”‚   â”‚   â”œâ”€â”€ generate_training_data.py
â”‚   â”‚   â”œâ”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ topics.py
â”‚   â”œâ”€â”€ plm_training/                 # Model fine-tuning
â”‚   â”‚   â”œâ”€â”€ distillbert.py
â”‚   â”‚   â””â”€â”€ roberta.py
â”‚   â”œâ”€â”€ scripts/                      # Batch scripts
â”‚   â””â”€â”€ results/                      # Training outputs
â”‚
â”œâ”€â”€ llm_direct_evaluation/             # Direct LLM evaluation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ evaluate.py               # Main evaluation script
â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”œâ”€â”€ scripts/                      # Batch evaluation
â”‚   â”œâ”€â”€ predictions/                  # Model predictions
â”‚   â””â”€â”€ results/                      # Evaluation metrics
â”‚
â””â”€â”€ data/                              # Evaluation datasets
    â”œâ”€â”€ clariq_preprocessed.tsv
    â”œâ”€â”€ ambignq_preprocessed.tsv
    â””â”€â”€ *_balanced_strict.tsv         # Synthetic training data
```

## ğŸ“„ License

[Add your license information here]

## ğŸ‘¥ Authors

[Add author information here]

## ğŸ“§ Contact

[Add contact information here]

## ğŸ™ Acknowledgments

- BEIR benchmark for evaluation datasets
- Meta AI for Llama models
- Google for Gemma models
- vLLM team for efficient inference framework
- TREC CAsT track for ClariQ dataset
- Google Research for AmbigNQ dataset

## ğŸ“š Citation

If you use this work, please cite:

```bibtex
[Add citation information when available]
```

---

**Last Updated**: December 2025
**Branch**: vllm
**Status**: Active Development
