# Comparison table (weighted avg values)

This table lists the weighted average precision, recall, F1, and mean inference latency (seconds) for both ClariQ and AmbigNQ per model and prompt. Avg F1 is the mean of ClariQ and AmbigNQ weighted F1.

**Selection rule:** Rows are sorted by Avg F1 (descending). The top row is suggested as the **baseline** for highest overall weighted F1.

|Method|Prompt|ClariQ P|ClariQ R|ClariQ F1|ClariQ Latency (s)|AmbigNQ P|AmbigNQ R|AmbigNQ F1|AmbigNQ Latency (s)|Avg F1|
|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|
|**Llama-3.3-70B**|few_shot|0.86|0.84|0.85|2.9260|0.54|0.57|0.53|2.9871|0.6900|
|Llama-3.3-70B|zero_shot|0.83|0.79|0.81|1.1926|0.53|0.55|0.54|1.1516|0.6750|
|Gemma-3-1B|few_shot|0.81|0.86|0.82|0.0247|0.52|0.56|0.50|0.0129|0.6600|
|Gemma-3-27B|zero_shot|0.80|0.77|0.79|0.1203|0.53|0.54|0.53|0.1248|0.6600|
|Llama-3.2-3B|zero_shot|0.84|0.88|0.84|0.0601|0.49|0.56|0.47|0.0567|0.6550|
|Gemma-3-1B|zero_shot|0.77|0.87|0.82|0.0076|0.50|0.55|0.49|0.0092|0.6550|
|Phi-3|few_shot|0.83|0.76|0.78|0.0688|0.53|0.56|0.51|0.0474|0.6450|
|Llama-3.2-1B|zero_shot|0.82|0.83|0.82|0.0293|0.49|0.56|0.47|0.0317|0.6450|
|Gemma-3-4B|few_shot|0.77|0.87|0.82|0.0685|0.50|0.58|0.43|0.0724|0.6250|
|Gemma-3-4B|zero_shot|0.77|0.88|0.82|0.0670|0.34|0.59|0.43|0.0399|0.6250|
|Phi-3|zero_shot|0.77|0.88|0.82|0.0782|0.34|0.59|0.43|0.0304|0.6250|
|Llama-3.2-1B|few_shot|0.77|0.88|0.82|0.0523|0.55|0.59|0.43|0.0353|0.6250|
|Gemma-3-27B|few_shot|0.84|0.68|0.73|0.2289|0.51|0.48|0.48|0.1816|0.6050|
|Llama-3.2-3B|few_shot|0.82|0.52|0.59|0.0798|0.49|0.44|0.42|0.0668|0.5050|
|Llama-3.1-8B|few_shot|0.86|0.44|0.52|0.0937|0.51|0.48|0.47|0.0718|0.4950|
|Llama-3.1-8B|zero_shot|0.81|0.42|0.50|0.0433|0.52|0.49|0.49|0.0466|0.4950|


### Notes
- Values are taken from the `weighted avg` row in each log's classification report and the `Mean:` line from inference time stats. Latency is in seconds.
- `Avg F1` is the simple mean of the ClariQ F1 and AmbigNQ F1 for a row.
- The top method (**Llama-3.3-70B few_shot**) is recommended as a **performance-first** baseline. If low latency is priority, `Gemma-3-1B few_shot` is a good latency-aware alternative (Avg F1 0.66 and mean latency â‰ˆ 0.02s).

---

## Pre-trained Language Model (PLM) Classifier Results

The table below shows results for fine-tuned PLM classifiers trained on LLM-generated embeddings. These classifiers are trained on embeddings from LLM predictions and provide a lightweight alternative to direct LLM inference.

|PLM Model|Embedding Source|ClariQ P|ClariQ R|ClariQ F1|ClariQ Latency (s)|AmbigNQ P|AmbigNQ R|AmbigNQ F1|AmbigNQ Latency (s)|Avg F1|Avg Latency(s)|
|---|---|---:|---:|---:|---:|---:|---:|---:|---:|---:|---:|
|RoBERTa|Llama-3.1-8B|0.81|0.82|0.81|0.0043|0.48|0.46|0.47|0.0040|0.6400|0.0042|
|DistilBERT|Llama-3.1-8B|0.83|0.78|0.80|0.0025|0.52|0.47|0.45|0.0025|0.6250|0.0025|
|DistilBERT|Llama-3.3-70B|0.83|0.76|0.79|0.0020|0.50|0.49|0.49|0.0020|0.6400|0.0020|
|RoBERTa|Llama-3.3-70B|0.82|0.53|0.60|0.0035|0.52|0.54|0.52|0.0032|0.5600|0.0034|
|DistilBERT|Gemma-3-27B|0.82|0.56|0.64|0.0020|0.57|0.44|0.34|0.0019|0.4900|0.0020|
|RoBERTa|Gemma-3-27B|0.83|0.37|0.43|0.0033|0.55|0.45|0.38|0.0031|0.4050|0.0032|

### Notes
- PLM classifiers are trained on embeddings generated by the specified LLM (Embedding Source).
- Values are taken from the `weighted avg` row in each PLM classification report.
- Latency values are per-sample average inference times.
- **RoBERTa + Llama-3.1-8B** and **DistilBERT + Llama-3.3-70B** both achieve the best Avg F1 (0.64) among PLM classifiers. DistilBERT + Llama-3.3-70B offers the lowest latency (0.0020s), making it the optimal choice for deployment scenarios requiring both high performance and fast inference.
- PLM classifiers offer 10-1000x speedup compared to direct LLM inference while maintaining reasonable performance.